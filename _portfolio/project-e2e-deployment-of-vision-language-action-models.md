---
title: "End-to-End Deployment of Vision-Language-Action Models"
layout: single
excerpt: "Led the end-to-end evaluation and deployment of a state-of-the-art Vision-Language-Action model, from literature review and benchmarking to building a physical robot setup, collecting demonstrations, and training the model for closed-loop control, laying the foundation to guide R&D activities"
category: work
order: 1
tags: [Generative AI, VLA, Benchmarking, Robotics]
---

## üìå Problem
Classical robotic automation struggles with complex, dexterous manipulation tasks common in industrial settings (e.g., pick-and-place, packaging, assembly). Emerging Vision-Language-Action (VLA) models promise more flexible, learning-based control, but their real-world feasibility and maturity for industrial deployment remain uncertain.

## üë§ Role
I led the end-to-end technical exploration: reviewing state-of-the-art VLA research, benchmarking candidate models in simulation, defining a concrete real-world use case, building the physical robot setup, collecting demonstrations via teleoperation, and training and evaluating the selected VLA model.

## üõ†Ô∏è Solution
We systematically evaluated leading VLAs on simulation benchmarks (e.g. LIBERO, DROID) and selected the pi0.5 model from Physical Intelligence. We then designed and deployed a real-world single-arm robotic setup with overhead and wrist cameras, collected task demonstrations, and trained the model to perform closed-loop control on the physical robot.

## üìà Impact
Although performance metrics such as task success rate and precision remain limited due to the immaturity of current VLA technology, the project demonstrated that a state-of-the-art VLA can be successfully deployed for real-world, closed-loop robotic control - providing a concrete proof of feasibility and a foundation to guide future R&D activities.
